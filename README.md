## Disaster Twitter classification
The purpose of that repository was to play with ðŸ¤— Hugging Face "transformers" and "datasets" libraries ([LINK](https://huggingface.co)) on the twitter disaster dataset ([LINK](https://www.kaggle.com/competitions/nlp-getting-started/overview)).

Solving the classification problem the DistilBERT base model obtained on test data F1 score equal to 0.84033 resulting in top 6% results in Kaggle "Getting Started" competition.
